{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa689086",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "- Saving all the CSVs with descriptive names in the Folder ./original_data\n",
    "- In case of XML Files the were converted to CSVs\n",
    "\n",
    "### Google Data:\n",
    "- Manuel removal of the first two rows and the region (Deutschland)\n",
    "- To get weekly Data it was necessary to download several 5 year intervals and normalize/stitching them together\n",
    "- Logarithmierung: Yes, because search interest data often exhibits exponential growth or decay, and log transformation helps stabilize variance and linearize trends.\n",
    "\n",
    "### AGPI & Gebrauchtwagenpreiseindex:\n",
    "- Only available on a monthly basis values were assigned to the last day of the month\n",
    "- Logarithmierung: Yes, because price indices can show multiplicative effects over time, and logs convert these to additive effects, stabilizing variance.\n",
    "\n",
    "### Arbeitslosenzahlen:\n",
    "- Loaded from CSV, cleaned numeric columns (removing thousands separators, converting to appropriate types).\n",
    "- 'Berichtsmonat' (reporting month) string converted to datetime objects, set to the last day of the month.\n",
    "- Logarithmierung: No, the raw figures were used directly as they represent counts and rates which are often analyzed in their original scale or after differencing for stationarity.\n",
    "\n",
    "### Euribor:\n",
    "- Data for 3-month, 6-month, and 12-month Euribor rates loaded from separate CSVs.\n",
    "- Unnecessary \"TIME PERIOD\" column dropped and columns renamed for clarity.\n",
    "- Dates were already in a usable monthly format.\n",
    "- Logarithmierung: No, interest rates are typically analyzed in their original percentage scale, especially when they can be near zero or negative, where logs are undefined or behave poorly.\n",
    "\n",
    "### ifo Beschäftigungsbarometer:\n",
    "- Loaded from CSV.\n",
    "- \"DATE\" column converted to datetime objects, set to the last day of the month, then formatted as string \"YYYY-MM-DD\".\n",
    "- Logarithmierung: No, index values like this are often used directly or differenced.\n",
    "\n",
    "### ifo Geschäftsklima:\n",
    "- Loaded from CSV.\n",
    "- \"DATE\" column (originally in \"MM/YYYY\" format) converted to datetime objects, set to the last day of the month, then formatted as string \"YYYY-MM-DD\".\n",
    "- Logarithmierung: No, similar to the Beschäftigungsbarometer, these index values are typically analyzed in their original scale.\n",
    "\n",
    "### BMW Sales:\n",
    "- Loaded from CSV, with specified separator and decimal character.\n",
    "- \"DATE\" column converted to datetime objects.\n",
    "- Logarithmierung: Yes, sales data can exhibit multiplicative seasonality and growth, log transformation helps in stabilizing variance and making the series more amenable to time series modeling.\n",
    "\n",
    "### Verbraucherpreisindex:\n",
    "- Loaded from CSV, with specified separator and decimal character.\n",
    "- Numeric columns representing various price indices cleaned (commas replaced with dots, converted to float).\n",
    "- \"Datum\" (Date) column converted to datetime objects, set to the last day of the month.\n",
    "- Logarithmierung: Yes, for the selected index columns, as price indices often have multiplicative components and log transformation helps in stabilizing variance and interpreting changes as percentage changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e11ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform helper functions\n",
    "def log_transform_series(series_to_log):\n",
    "    return np.log(series_to_log.replace(0, np.nan))\n",
    "\n",
    "\n",
    "def create_log_transformed_df(\n",
    "    input_df, value_col_name, new_log_col_name, date_col_name=\"DATE\"\n",
    "):\n",
    "    df_log = pd.DataFrame()\n",
    "    df_log[date_col_name] = input_df[date_col_name]\n",
    "    df_log[new_log_col_name] = log_transform_series(input_df[value_col_name])\n",
    "    return df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4af4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# google trends\n",
    "def merge_and_normalize_trends(dfs, col_name):\n",
    "    # Start with the first DataFrame\n",
    "    result = dfs[0].copy()\n",
    "    for next_df in dfs[1:]:\n",
    "        # Find overlap\n",
    "        overlap = set(result[\"Woche\"]) & set(next_df[\"Woche\"])\n",
    "        if overlap:\n",
    "            # Use the mean ratio in the overlap to normalize\n",
    "            overlap = list(overlap)\n",
    "            result_overlap = result[result[\"Woche\"].isin(overlap)].set_index(\"Woche\")\n",
    "            next_overlap = next_df[next_df[\"Woche\"].isin(overlap)].set_index(\"Woche\")\n",
    "            ratio = result_overlap[col_name].mean() / next_overlap[col_name].mean()\n",
    "        else:\n",
    "            ratio = 1.0\n",
    "        # Normalize next_df\n",
    "        next_df_norm = next_df.copy()\n",
    "        next_df_norm[col_name] = next_df_norm[col_name] * ratio\n",
    "        # Append only non-overlapping part\n",
    "        non_overlap = ~next_df_norm[\"Woche\"].isin(result[\"Woche\"])\n",
    "        result = pd.concat([result, next_df_norm[non_overlap]], ignore_index=True)\n",
    "    # Sort by date\n",
    "    result = result.sort_values(\"Woche\").reset_index(drop=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Keyword: \"Auto Finanzierung\"\n",
    "gt_auto_finanzierung_1 = pd.read_csv(\"./original_data/gt_auto_finanzierung_1.csv\")\n",
    "gt_auto_finanzierung_2 = pd.read_csv(\"./original_data/gt_auto_finanzierung_2.csv\")\n",
    "gt_auto_finanzierung_3 = pd.read_csv(\"./original_data/gt_auto_finanzierung_3.csv\")\n",
    "\n",
    "gt_auto_finanzierung = merge_and_normalize_trends(\n",
    "    [gt_auto_finanzierung_1, gt_auto_finanzierung_2, gt_auto_finanzierung_3],\n",
    "    col_name=\"Auto Finanzierung\",\n",
    ")\n",
    "gt_auto_finanzierung.rename(columns={\"Woche\": \"DATE\"}, inplace=True)\n",
    "gt_auto_finanzierung[\"DATE\"] = pd.to_datetime(gt_auto_finanzierung[\"DATE\"])\n",
    "# Keyword: \"BMW Finanzierung\"\n",
    "gt_bmw_finanzierung_1 = pd.read_csv(\"./original_data/gt_bmw_finanzierung_1.csv\")\n",
    "gt_bmw_finanzierung_2 = pd.read_csv(\"./original_data/gt_bmw_finanzierung_2.csv\")\n",
    "gt_bmw_finanzierung_3 = pd.read_csv(\"./original_data/gt_bmw_finanzierung_3.csv\")\n",
    "\n",
    "gt_bmw_finanzierung = merge_and_normalize_trends(\n",
    "    [gt_bmw_finanzierung_1, gt_bmw_finanzierung_2, gt_bmw_finanzierung_3],\n",
    "    col_name=\"BMW Finanzierung\",\n",
    ")\n",
    "gt_bmw_finanzierung.rename(columns={\"Woche\": \"DATE\"}, inplace=True)\n",
    "gt_bmw_finanzierung[\"DATE\"] = pd.to_datetime(gt_bmw_finanzierung[\"DATE\"])\n",
    "# Keyword: \"Auto Leasing\"\n",
    "gt_auto_leasing_1 = pd.read_csv(\"./original_data/gt_auto_leasing_1.csv\")\n",
    "gt_auto_leasing_2 = pd.read_csv(\"./original_data/gt_auto_leasing_2.csv\")\n",
    "gt_auto_leasing_3 = pd.read_csv(\"./original_data/gt_auto_leasing_3.csv\")\n",
    "\n",
    "gt_auto_leasing = merge_and_normalize_trends(\n",
    "    [gt_auto_leasing_1, gt_auto_leasing_2, gt_auto_leasing_3], col_name=\"Auto Leasing\"\n",
    ")\n",
    "gt_auto_leasing.rename(columns={\"Woche\": \"DATE\"}, inplace=True)\n",
    "gt_auto_leasing[\"DATE\"] = pd.to_datetime(gt_auto_leasing[\"DATE\"])\n",
    "# Keyword: \"BMW Leasing\"\n",
    "gt_bmw_leasing_1 = pd.read_csv(\"./original_data/gt_bmw_leasing_1.csv\")\n",
    "gt_bmw_leasing_2 = pd.read_csv(\"./original_data/gt_bmw_leasing_2.csv\")\n",
    "gt_bmw_leasing_3 = pd.read_csv(\"./original_data/gt_bmw_leasing_3.csv\")\n",
    "gt_bmw_leasing = merge_and_normalize_trends(\n",
    "    [gt_bmw_leasing_1, gt_bmw_leasing_2, gt_bmw_leasing_3], col_name=\"BMW Leasing\"\n",
    ")\n",
    "gt_bmw_leasing.rename(columns={\"Woche\": \"DATE\"}, inplace=True)\n",
    "gt_bmw_leasing[\"DATE\"] = pd.to_datetime(gt_bmw_leasing[\"DATE\"])\n",
    "# Keyword: \"Leasing\"\n",
    "gt_leasing_1 = pd.read_csv(\"./original_data/gt_leasing_1.csv\")\n",
    "gt_leasing_2 = pd.read_csv(\"./original_data/gt_leasing_2.csv\")\n",
    "gt_leasing_3 = pd.read_csv(\"./original_data/gt_leasing_3.csv\")\n",
    "gt_leasing = merge_and_normalize_trends(\n",
    "    [gt_leasing_1, gt_leasing_2, gt_leasing_3], col_name=\"leasing\"\n",
    ")\n",
    "gt_leasing.rename(columns={\"Woche\": \"DATE\"}, inplace=True)\n",
    "gt_leasing[\"DATE\"] = pd.to_datetime(gt_leasing[\"DATE\"])\n",
    "# Keyword: \"BMW\"\n",
    "gt_bmw_1 = pd.read_csv(\"./original_data/gt_bmw_1.csv\")\n",
    "gt_bmw_2 = pd.read_csv(\"./original_data/gt_bmw_2.csv\")\n",
    "gt_bmw_3 = pd.read_csv(\"./original_data/gt_bmw_3.csv\")\n",
    "gt_bmw = merge_and_normalize_trends([gt_bmw_1, gt_bmw_2, gt_bmw_3], col_name=\"BMW\")\n",
    "gt_bmw.rename(columns={\"Woche\": \"DATE\"}, inplace=True)\n",
    "gt_bmw[\"DATE\"] = pd.to_datetime(gt_bmw[\"DATE\"])\n",
    "\n",
    "\n",
    "# Log transform\n",
    "gt_bmw_leasing_log = create_log_transformed_df(\n",
    "    gt_bmw_leasing, \"BMW Leasing\", \"BMWLeasing\"\n",
    ")\n",
    "gt_bmw_finanzierung_log = create_log_transformed_df(\n",
    "    gt_bmw_finanzierung, \"BMW Finanzierung\", \"BMWFinanzierung\"\n",
    ")\n",
    "gt_auto_finanzierung_log = create_log_transformed_df(\n",
    "    gt_auto_finanzierung, \"Auto Finanzierung\", \"AutoFinanzierung\"\n",
    ")\n",
    "gt_auto_leasing_log = create_log_transformed_df(\n",
    "    gt_auto_leasing, \"Auto Leasing\", \"AutoLeasing\"\n",
    ")\n",
    "gt_leasing_log = create_log_transformed_df(gt_leasing, \"leasing\", \"leasing\")\n",
    "gt_bmw_log = create_log_transformed_df(gt_bmw, \"BMW\", \"BMW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593508f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agpi\n",
    "agpi = pd.read_csv(\"./original_data/agpi.csv\")\n",
    "\n",
    "agpi[\"DATE\"] = pd.to_datetime(agpi[\"DATE\"], format=\"%Y-%m\") + pd.offsets.MonthEnd(0)\n",
    "# agpi[\"DATE\"] = agpi[\"DATE\"].dt.strftime(\"%Y-%m-%d\") # Keep as datetime\n",
    "\n",
    "agpi_log = create_log_transformed_df(agpi, \"AGPI\", \"AGPI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5797f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gebrauchtwagenpreiseindex\n",
    "\n",
    "gebrauchtwagenpreisindex = pd.read_csv(\"./original_data/gebrauchtwagenpreisindex.csv\")\n",
    "gebrauchtwagenpreisindex[\"DATE\"] = pd.to_datetime(\n",
    "    gebrauchtwagenpreisindex[\"DATE\"], errors=\"coerce\"\n",
    ") + pd.offsets.MonthEnd(0)\n",
    "# gebrauchtwagenpreisindex[\"DATE\"] = gebrauchtwagenpreisindex[\"DATE\"].dt.strftime( # Keep as datetime\n",
    "#     \"%Y-%m-%d\"\n",
    "# )\n",
    "\n",
    "gebrauchtwagenpreisindex_log = create_log_transformed_df(\n",
    "    gebrauchtwagenpreisindex, \"Gebrauchtwagenpreisindex\", \"Gebrauchtwagenpreisindex\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc76135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbeitslosenzahlen\n",
    "\n",
    "month_map = {\n",
    "    \"Januar\": \"01\",\n",
    "    \"Februar\": \"02\",\n",
    "    \"März\": \"03\",\n",
    "    \"April\": \"04\",\n",
    "    \"Mai\": \"05\",\n",
    "    \"Juni\": \"06\",\n",
    "    \"Juli\": \"07\",\n",
    "    \"August\": \"08\",\n",
    "    \"September\": \"09\",\n",
    "    \"Oktober\": \"10\",\n",
    "    \"November\": \"11\",\n",
    "    \"Dezember\": \"12\",\n",
    "}\n",
    "arbeitslosenquote = pd.read_csv(\n",
    "    \"./original_data/arbeitslosenzahlen.csv\", sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "# Clean integer columns (remove dots, convert to int)\n",
    "for col in [\"Bestand Arbeitslose\", \"Zugang Arbeitslose\", \"Abgang Arbeitslose\"]:\n",
    "    arbeitslosenquote[col] = (\n",
    "        arbeitslosenquote[col]\n",
    "        .astype(str)\n",
    "        .str.replace(\".\", \"\", regex=False)\n",
    "        .replace(\"\", \"0\")\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "# Clean float column (replace comma with dot, convert to float)\n",
    "arbeitslosenquote[\"Arbeitslosenquote\"] = (\n",
    "    arbeitslosenquote[\"Arbeitslosenquote\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\",\", \".\", regex=False)\n",
    "    .replace(\"\", \"0\")\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# Convert 'Berichtsmonat' to datetime\n",
    "arbeitslosenquote[\"DATE\"] = (\n",
    "    arbeitslosenquote[\"Berichtsmonat\"]\n",
    "    .astype(str)\n",
    "    .apply(\n",
    "        lambda x: pd.to_datetime(\n",
    "            f\"{x.split(' ')[1]}-{month_map.get(x.split(' ')[0], '01')}-01\",\n",
    "            errors=\"coerce\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "arbeitslosenquote[\"DATE\"] = arbeitslosenquote[\"DATE\"] + pd.offsets.MonthEnd(0)\n",
    "\n",
    "# Reorder columns\n",
    "arbeitslosenquote = arbeitslosenquote.drop(columns=[\"Berichtsmonat\"])\n",
    "arbeitslosenquote = arbeitslosenquote[\n",
    "    [\"DATE\"] + [col for col in arbeitslosenquote.columns if col != \"DATE\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# euribor\n",
    "# 3 months\n",
    "euribor_3m = pd.read_csv(\"./original_data/euribor_3m.csv\")\n",
    "euribor_3m = euribor_3m.drop(columns=[\"TIME PERIOD\"])\n",
    "euribor_3m = euribor_3m.rename(\n",
    "    columns={\n",
    "        \"Euribor 3-month - Historical close, average of observations through period (FM.M.U2.EUR.RT.MM.EURIBOR3MD_.HSTA)\": \"Euribor 3M\"\n",
    "    }\n",
    ")\n",
    "# 6 months\n",
    "euribor_6m = pd.read_csv(\"./original_data/euribor_6m.csv\")\n",
    "euribor_6m = euribor_6m.drop(columns=[\"TIME PERIOD\"])\n",
    "euribor_6m = euribor_6m.rename(\n",
    "    columns={\n",
    "        \"Euribor 6-month - Historical close, average of observations through period (FM.M.U2.EUR.RT.MM.EURIBOR6MD_.HSTA)\": \"Euribor 6M\"\n",
    "    }\n",
    ")\n",
    "# 12 months\n",
    "euribor_12m = pd.read_csv(\"./original_data/euribor_12m.csv\")\n",
    "euribor_12m = euribor_12m.drop(columns=[\"TIME PERIOD\"])\n",
    "euribor_12m = euribor_12m.rename(\n",
    "    columns={\n",
    "        \"Euribor 1-year - Historical close, average of observations through period (FM.M.U2.EUR.RT.MM.EURIBOR1YD_.HSTA)\": \"Euribor 12M\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657281c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ifo beschäftigungsbarometer\n",
    "beschaeftigungsbarometer = pd.read_csv(\n",
    "    \"./original_data/ifo_beschaeftigungsbarometer.csv\"\n",
    ")\n",
    "beschaeftigungsbarometer[\"DATE\"] = pd.to_datetime(\n",
    "    beschaeftigungsbarometer[\"DATE\"], errors=\"coerce\"\n",
    ") + pd.offsets.MonthEnd(0)\n",
    "beschaeftigungsbarometer[\"DATE\"] = beschaeftigungsbarometer[\"DATE\"].dt.strftime(\n",
    "    \"%Y-%m-%d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd1d515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ifo geschäftsklima\n",
    "geschaeftsklima = pd.read_csv(\"./original_data/ifo_geschaeftsklima.csv\")\n",
    "geschaeftsklima[\"DATE\"] = pd.to_datetime(\n",
    "    geschaeftsklima[\"DATE\"].str.strip(), format=\"%m/%Y\"\n",
    ") + pd.offsets.MonthEnd(0)\n",
    "geschaeftsklima[\"DATE\"] = geschaeftsklima[\"DATE\"].dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bmw sales\n",
    "bmw_sales = pd.read_csv(\"./original_data/bmw_sales.csv\", sep=\";\", decimal=\",\")\n",
    "bmw_sales[\"DATE\"] = pd.to_datetime(bmw_sales[\"DATE\"])  # Ensure DATE is datetime\n",
    "\n",
    "bmw_sales_log = create_log_transformed_df(bmw_sales, \"SALES\", \"SALES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5789228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbraucherpreisindex\n",
    "verbraucherpreisindex = pd.read_csv(\n",
    "    \"./original_data/verbraucherpreisindex.csv\", sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "# Columns to clean: replace comma with period and convert to float\n",
    "cols_to_clean_comma = [\n",
    "    \"Index der Erzeugerpreise gewerblicher Produkte\",\n",
    "    \"Index der Einfuhrpreise\",\n",
    "    \"Index der Ausfuhrpreise\",\n",
    "]\n",
    "\n",
    "for col in cols_to_clean_comma:\n",
    "    verbraucherpreisindex[col] = (\n",
    "        verbraucherpreisindex[col]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "        .replace(\n",
    "            \".\", \"0\", regex=False\n",
    "        )  # Handle cases where only a period might be left or as a placeholder\n",
    "        .replace(\"\", \"0\")  # Replace empty strings with 0\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "# Convert \"Datum\" to datetime, then set to last day of month\n",
    "verbraucherpreisindex[\"DATE\"] = pd.to_datetime(\n",
    "    verbraucherpreisindex[\"Datum\"], format=\"%d/%m/%Y\"\n",
    ") + pd.offsets.MonthEnd(0)\n",
    "# verbraucherpreisindex[\"DATE\"] = verbraucherpreisindex[\"DATE\"].dt.strftime(\"%Y-%m-%d\") # Keep as datetime\n",
    "verbraucherpreisindex = verbraucherpreisindex.drop(columns=[\"Datum\"])\n",
    "cols = [\"DATE\"] + [col for col in verbraucherpreisindex.columns if col != \"DATE\"]\n",
    "verbraucherpreisindex = verbraucherpreisindex[cols]\n",
    "\n",
    "# Log-transform selected columns (except DATE)\n",
    "cols_to_log = [\n",
    "    \"Verbraucherpreisindex\",\n",
    "    \"Index der Einzelhandelspreise\",\n",
    "    \"Index der Erzeugerpreise gewerblicher Produkte\",\n",
    "    \"Index der Growsshandelsverkaufspreise\",\n",
    "    \"Index der Einfuhrpreise\",\n",
    "    \"Index der Ausfuhrpreise\",\n",
    "]\n",
    "\n",
    "verbraucherpreisindex_log = verbraucherpreisindex[[\"DATE\"] + cols_to_log].copy()\n",
    "for col in cols_to_log:\n",
    "    verbraucherpreisindex_log[col] = log_transform_series(\n",
    "        verbraucherpreisindex_log[col]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96581b40",
   "metadata": {},
   "source": [
    "## Data Merging, Interpolation and converting to a weekly-basis\n",
    "\n",
    "### Merging\n",
    "\n",
    "- Array with all the dataframes, which should be merged\n",
    "- Using lambda function tou outer join all the dataframes using the DATE as a index\n",
    "- Exporting merged dataframe to data.csv\n",
    "- Describing the Data to gather more insights\n",
    "\n",
    "### Interpolation\n",
    "\n",
    "- Filling missing Values using interpolation via the time method\n",
    "\n",
    "### Weekly-basis\n",
    "\n",
    "- Using the Sunday of each Week as a common Basis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e47d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes on DATE\n",
    "from functools import reduce\n",
    "\n",
    "dataframes = [\n",
    "    bmw_sales_log,\n",
    "    gt_auto_finanzierung_log,\n",
    "    gt_bmw_finanzierung_log,\n",
    "    gt_auto_leasing_log,\n",
    "    gt_bmw_leasing_log,\n",
    "    gt_leasing_log,\n",
    "    gt_bmw_log,\n",
    "    agpi_log,\n",
    "    gebrauchtwagenpreisindex_log,\n",
    "    euribor_3m,\n",
    "    euribor_6m,\n",
    "    euribor_12m,\n",
    "    verbraucherpreisindex_log,\n",
    "    arbeitslosenquote,\n",
    "    beschaeftigungsbarometer,\n",
    "    geschaeftsklima,\n",
    "]\n",
    "\n",
    "for df_item in dataframes:\n",
    "    if \"DATE\" in df_item.columns:\n",
    "        df_item[\"DATE\"] = pd.to_datetime(df_item[\"DATE\"])\n",
    "\n",
    "df_merged = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=\"DATE\", how=\"outer\"), dataframes\n",
    ")\n",
    "df_merged.columns = df_merged.columns.str.replace(r\"\\s+\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29becb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92e4d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation\n",
    "\n",
    "# Ensure 'DATE' column is datetime\n",
    "df_merged[\"DATE\"] = pd.to_datetime(df_merged[\"DATE\"])\n",
    "df_merged = df_merged.sort_values(\"DATE\").reset_index(drop=True)\n",
    "\n",
    "# Interpolate on the original merged data\n",
    "interpolated_on_original_timeline = (\n",
    "    df_merged.set_index(\"DATE\")\n",
    "    .interpolate(method=\"time\", limit_direction=\"both\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Weekly (Sunday) index\n",
    "all_sundays_index = pd.date_range(\n",
    "    start=df_merged[\"DATE\"].min(), end=df_merged[\"DATE\"].max(), freq=\"W-SUN\", name=\"DATE\"\n",
    ")\n",
    "\n",
    "# Align the broadly interpolated data to the target weekly (Sunday) index\n",
    "interpolated_on_original_timeline = interpolated_on_original_timeline.set_index(\"DATE\")\n",
    "\n",
    "# Reindex to the all_sundays_index\n",
    "data_aligned_to_sundays = interpolated_on_original_timeline.reindex(all_sundays_index)\n",
    "\n",
    "# Interpolate again to fill any NaNs created by reindexing to Sundays\n",
    "df_interpolated = data_aligned_to_sundays.interpolate(\n",
    "    method=\"time\", limit_direction=\"both\"\n",
    ").reset_index()\n",
    "\n",
    "final_start_date = pd.Timestamp(\"2015-01-01\")\n",
    "df_interpolated = df_interpolated[df_interpolated[\"DATE\"] >= final_start_date].reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d110c12c",
   "metadata": {},
   "source": [
    "## Seasonal adjustment using STL\n",
    "\n",
    "- Defining, which columns should be saisonal adjusted\n",
    "- Using 52 as period, beause 52 weeks are in a year --> repetition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e925556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonal adjustment\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# List of columns to be seasonally adjusted\n",
    "seasonal_cols = [\n",
    "    \"SALES\",\n",
    "    \"AutoFinanzierung\",\n",
    "    \"BMWFinanzierung\",\n",
    "    \"AutoLeasing\",\n",
    "    \"BMWLeasing\",\n",
    "    \"leasing\",\n",
    "    \"BMW\",\n",
    "    \"AGPI\",\n",
    "    \"Gebrauchtwagenpreisindex\",\n",
    "    \"Euribor3M\",\n",
    "    \"Euribor6M\",\n",
    "    \"Euribor12M\",\n",
    "    \"BestandArbeitslose\",\n",
    "    \"ZugangArbeitslose\",\n",
    "    \"AbgangArbeitslose\",\n",
    "    \"Arbeitslosenquote\",\n",
    "    \"Beschäftigungsbarometer\",\n",
    "    \"VerarbeitendesGewerbe\",\n",
    "    \"Bauhauptgewerbe\",\n",
    "    \"Handel\",\n",
    "    \"Dienstleistungssektor\",\n",
    "    \"Geschäftsklima\",\n",
    "    \"Geschäftslage\",\n",
    "    \"Geschäftserwartungen\",\n",
    "    \"Konjunkturampel\",\n",
    "]\n",
    "\n",
    "df_sa = df_interpolated.copy()\n",
    "df_sa = df_sa.set_index(\"DATE\")\n",
    "\n",
    "for col in seasonal_cols:\n",
    "    if col in df_sa.columns:\n",
    "        series = df_sa[col].dropna()\n",
    "        if len(series) > 2 * 52:  # Check if enough data points (period=52 weeks)\n",
    "            stl = STL(series, period=52, robust=True)\n",
    "            res = stl.fit()\n",
    "            df_sa[col] = pd.Series(res.trend + res.resid, index=series.index)\n",
    "df_sa = df_sa.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17181b9",
   "metadata": {},
   "source": [
    "## Plotting of the time series to gather more insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c11f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting Time Series\n",
    "# def plot_time_series(df, title, x_col=\"DATE\", y_cols=None):\n",
    "#     import matplotlib.dates as mdates\n",
    "\n",
    "#     if y_cols is None:\n",
    "#         y_cols = [col for col in df.columns if col != x_col]  # All columns except x_col\n",
    "\n",
    "#     # Ensure x_col is datetime\n",
    "#     df_plot = df.copy()\n",
    "#     if not pd.api.types.is_datetime64_any_dtype(df_plot[x_col]):\n",
    "#         df_plot[x_col] = pd.to_datetime(df_plot[x_col])\n",
    "\n",
    "#     for col in y_cols:\n",
    "#         if col in df_plot.columns:  # Check if column exists\n",
    "#             plt.figure(figsize=(8, 4))\n",
    "#             plt.plot(df_plot[x_col], df_plot[col], label=col)\n",
    "#             plt.title(f\"{title} - {col}\")\n",
    "#             plt.xlabel(\"Date\")\n",
    "#             plt.ylabel(\"Value\")\n",
    "\n",
    "#             # Set x-axis major ticks to yearly\n",
    "#             ax = plt.gca()\n",
    "#             ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "#             ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "\n",
    "#             plt.legend()\n",
    "#             plt.grid(True)\n",
    "#             plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "#             plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
    "#             plt.show()\n",
    "#         else:\n",
    "#             print(f\"Warning: Column '{col}' not found in DataFrame.\")\n",
    "\n",
    "\n",
    "# plot_time_series(df_sa, \"Time Series of SA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909cb48f",
   "metadata": {},
   "source": [
    "## Testing the stationarity of the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers functions Functions\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "\n",
    "# ADF Test for Stationarity\n",
    "def is_stationary_adf(series, significance_level=0.05, regression=\"ct\"):\n",
    "    result = adfuller(series, autolag=\"AIC\", regression=regression)\n",
    "    p_value = result[1]\n",
    "    return p_value < significance_level\n",
    "\n",
    "\n",
    "# KPSS Test for Stationarity\n",
    "def is_stationary_kpss(series, significance_level=0.05, regression=\"ct\"):\n",
    "    result = kpss(series, regression=regression, nlags=\"auto\")\n",
    "    p_value = result[1]\n",
    "    if p_value < significance_level:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "# Overall Stationarity Test\n",
    "def overall_stationarity_test(df_series):\n",
    "    stationary = []\n",
    "    non_stationary = []\n",
    "    need_further_check = []\n",
    "\n",
    "    for column in df_series.columns:\n",
    "        if column == \"DATE\":\n",
    "            continue\n",
    "\n",
    "        adf_result = is_stationary_adf(df_series[column])\n",
    "        kpss_result = is_stationary_kpss(df_series[column])\n",
    "\n",
    "        if adf_result and kpss_result:\n",
    "            stationary.append(column)\n",
    "        elif not adf_result and not kpss_result:\n",
    "            non_stationary.append(column)\n",
    "        else:\n",
    "            need_further_check.append(column)\n",
    "    return {\n",
    "        \"stationary\": stationary,\n",
    "        \"non_stationary\": non_stationary,\n",
    "        \"need_further_check\": need_further_check,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f45eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing all columns for stationarity\n",
    "stationarity_results = overall_stationarity_test(df_sa)\n",
    "\n",
    "# Print results\n",
    "print(stationarity_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd4fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differencing Helper Function\n",
    "def difference_df_columns(input_df, columns_to_difference, periods=1):\n",
    "    df_out = input_df.copy()\n",
    "    for col in columns_to_difference:\n",
    "        col in df_out.columns:\n",
    "        df_out[col] = df_out[col].diff(periods=periods)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differencing the non-stationary columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Retesting for Stationarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ae5463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to difference specified columns in a DataFrame\n",
    "\n",
    "\n",
    "df_sa = difference_df_columns(df_sa, cols_to_make_stationary, periods=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c065553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # # Plotting after Stationarity Tests\n",
    "\n",
    "  # # Plotting Time Series\n",
    "  # def plot_time_series(df, title, x_col=\"DATE\", y_cols=None):\n",
    "  #     import matplotlib.dates as mdates\n",
    "\n",
    "  #     if y_cols is None:\n",
    "  #         y_cols = [col for col in df.columns if col != x_col]  # All columns except x_col\n",
    "\n",
    "  #     # Ensure x_col is datetime\n",
    "  #     df_plot = df.copy()\n",
    "  #     if not pd.api.types.is_datetime64_any_dtype(df_plot[x_col]):\n",
    "  #         df_plot[x_col] = pd.to_datetime(df_plot[x_col])\n",
    "\n",
    "  #     for col in y_cols:\n",
    "  #         if col in df_plot.columns:  # Check if column exists\n",
    "  #             plt.figure(figsize=(8, 4))\n",
    "  #             plt.plot(df_plot[x_col], df_plot[col], label=col)\n",
    "  #             plt.title(f\"{title} - {col}\")\n",
    "  #             plt.xlabel(\"Date\")\n",
    "  #             plt.ylabel(\"Value\")\n",
    "\n",
    "  #             # Set x-axis major ticks to yearly\n",
    "  #             ax = plt.gca()\n",
    "  #             ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "  #             ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "\n",
    "  #             plt.legend()\n",
    "  #             plt.grid(True)\n",
    "  #             plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "  #             plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
    "  #             plt.show()\n",
    "  #         else:\n",
    "  #             print(f\"Warning: Column '{col}' not found in DataFrame.\")\n",
    "\n",
    "\n",
    "  # plot_time_series(df_sa, \"Time Series of SA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca8ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "df_fe = df_sa.copy()\n",
    "df_fe[\"DATE\"] = pd.to_datetime(df_fe[\"DATE\"])\n",
    "\n",
    "# Create forward-lagged features for SALES at every 4 weeks up to 52 weeks\n",
    "for lag in range(4, 53, 4):\n",
    "    df_fe[f\"SALES_t+{lag}\"] = df_fe[\"SALES\"].shift(-lag)\n",
    "\n",
    "# Move the lagged SALES columns to be right after the SALES column\n",
    "lagged_cols = [f\"SALES_t+{lag}\" for lag in range(4, 53, 4)]\n",
    "cols = (\n",
    "    [\"DATE\", \"SALES\"]\n",
    "    + lagged_cols\n",
    "    + [col for col in df_fe.columns if col not in ([\"DATE\", \"SALES\"] + lagged_cols)]\n",
    ")\n",
    "df_fe = df_fe[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742d4ff",
   "metadata": {},
   "source": [
    "## Pearson & Spearman corrolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc04282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_cols = [\"SALES\"] + [f\"SALES_t+{i}\" for i in range(4, 53, 4)]\n",
    "other_cols = [col for col in df_fe.columns if col not in target_cols + [\"DATE\"]]\n",
    "\n",
    "correlation_methods = [\"pearson\", \"spearman\"]\n",
    "for method in correlation_methods:\n",
    "    correlation_matrix = df_fe[target_cols + other_cols].corr(method=method)\n",
    "    correlation_subset = correlation_matrix.loc[other_cols, target_cols]\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_context(\"paper\", font_scale=1.4)\n",
    "    custom_palette = sns.diverging_palette(\n",
    "        240, 10, s=80, l=55, n=256, center=\"light\", as_cmap=True\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    sns.heatmap(\n",
    "        correlation_subset,\n",
    "        annot=True,\n",
    "        cmap=custom_palette,\n",
    "        fmt=\".2f\",\n",
    "        vmin=-0.75,\n",
    "        vmax=0.75,\n",
    "        center=0,\n",
    "        annot_kws={\"size\": 12},\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8},\n",
    "    )\n",
    "\n",
    "    plt.title(\n",
    "        f\"{method.capitalize()} Correlation Heatmap: SALES (and lags) vs. Other Variables\",\n",
    "        fontsize=20,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79851e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECM\n",
    "from statsmodels.tsa.vector_ar.vecm import VECM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
